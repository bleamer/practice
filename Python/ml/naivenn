import numpy as np
import sklearn
from sklearn import datasets, linear_model
import matplotlib.pyplot as plt


print_freq = 1000

def plot_decision_boundary(pred_func, X, y):
	# Set the min and max values and give it some padding
	x_min, x_max = X[:,0].min()-.5, X[:,0].max()+.5
	y_min, y_max = X[:,1].min()-.5, X[:,1].max()+.5
	h = 0.01

	# Generate grid points
	xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
	
	# Predict the grid ranges
	Z = pred_func(np.c_[xx.ravel(), yy.ravel()])
	Z = Z.reshape(xx.shape)


	# Plot the contour and training examples
	plt.contour(xx, yy, Z, cmap=plt.cm.Spectral)
	plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.Spectral)
	# plt.show()


# Generating random set
np.random.seed(0)
# Use make_moons function to create random data
X, y = sklearn.datasets.make_moons(200, noise=0.20)
# plt.scatter(X[:,1], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)


# train for logistic regression
clf = sklearn.linear_model.LogisticRegressionCV()
clf.fit(X,y)

# Plot the decision boundary
plot_decision_boundary(lambda x: clf.predict(x), X, y)
# plt.title("Logistic Regression")


# Try Neural network now
num_examples = len(X)	# training data size
nn_input_dim = 2		# No. of input nodes
nn_output_dim= 2		# No. of output nodes / classes

# Gradient descent paramaters
epsilon = 0.01			# Learning rate factor
reg_lamda = 0.01		# Regularization strength


# Calulating loss function / the error in predicted output vs actual output
def calculate_loss(model):
	W1, b1, W2, b2 = model['W1'],model['b1'],model['W2'],model['b2']

	# forward propagation for our prediction
	z1 = X.dot(W1) + b1
	a1 = np.tanh(z1)
	
	z2 = X.dot(W2) + b2

	# Expected scores
	exp_scores = np.exp(z2)
	probs = exp_scores / np.sum (exp_scores, axis=1, keepdims=True)

	# Calculating the loss
	correct_logprobs = - np.log(probs[range(num_examples),y])
	data_loss = np.sum(correct_logprobs)

	# Adding regularization  term to loss
	data_loss += reg_lamda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))
	return 1./num_examples * data_loss # averaging over number of samples



# Function to predict class
def predict(model, x):
	W1, b1, W2, b2 = model['W1'],model['b1'],model['W2'],model['b2']

	# forward propagation for our prediction
	z1 = x.dot(W1) + b1
	a1 = np.tanh(z1)
	
	z2 = a1.dot(W2) + b2

	# Expected scores
	exp_scores = np.exp(z2)
	probs = exp_scores / np.sum (exp_scores, axis=1, keepdims=True)

	return np.argmax(probs, axis=1)



# Make the model based on nn
# nn_hdim : Number of nodes in hidden layer
# num_passes: Number of passes through the training data for gradient descent
# print_loss : Flag to enable printing losses based on iterations
def build_model(nn_hdim, num_passes = 2000, print_loss = False):
	# Initilize the parameter to random values. 
	np.random.seed(0)
	W1 = np.random.randn(nn_input_dim, nn_hdim)/np.sqrt(nn_input_dim)
	b1 = np.zeros((1, nn_hdim))
	W2 = np.random.randn(nn_hdim, nn_output_dim)/np.sqrt(nn_hdim)
	b2 = np.zeros((1, nn_output_dim))

	# initialize the model
	model = {}

	# Gradient descent 
	for i in xrange(0, num_passes):

		# Forwared propagation
		z1 = X.dot(W1) + b1
		a1 = np.tanh(z1)
		z2 = a1.dot(W2) + b2
		exp_scores = np.exp(z2)
		probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)

		# Backwards propagation
		delta3 = probs
		delta3[range(num_examples),y] -= 1
		dW2 = (a1.T).dot(delta3)
		db2 = np.sum(delta3, axis=0, keepdims=True)
		delta2 = delta3.dot(W2.T) * (1-np.power(a1,2))
		dW1 = np.dot(X.T, delta2)
		db1 = np.sum(delta2, axis=0)

		# Add regularization terms
		dW2 += reg_lamda * W2
		dW1 += reg_lamda * W1

		# Gradient descent parameter update
		W1 += -epsilon * dW1
		b1 += -epsilon * db1
		W2 += -epsilon * dW2
		b2 += -epsilon * db2

		# Assign new parameters to the model
		model = { 'W1':W1, 'b1':b1,'W2':W2, 'b2':b2}

		# Print the loss
		if print_loss and i % print_freq == 0:
			print "Loss after iteration %i: %f" %(i, calculate_loss(model))

	return model

# build an n hidden layer model
model = build_model(3)

#Plot the decision boundary
plot_decision_boundary(lambda x:predict(model, x), X,y)
plt.title("NN boundary")


plt.figure(figsize=(16,32))
hidden_layer_dimensions = [1,2,3,4,5,20,50]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
	plt.subplot(5, 2, i+1)
	plt.title('Hidden Layer size %d' % nn_hdim)
	model = build_model(nn_hdim)
	plot_decision_boundary(lambda x:predict(model, x), X, y)


plt.show()
